{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from mono.model.monodepth_model import get_configured_monodepth_model\n",
    "from mono.utils.running import load_ckpt\n",
    "try:\n",
    "    from mmcv.utils import Config\n",
    "    # from mmcv.utils import Config, DictAction\n",
    "except:\n",
    "    from mmengine import Config\n",
    "    # from mmengine import Config, DictAction\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from mono.utils.do_test import transform_test_data_scalecano, get_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: No module named 'triton'\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DepthModel(\n",
       "  (depth_model): DensePredModel(\n",
       "    (encoder): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): BlockChunk(\n",
       "          (0-23): 24 x Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (decoder): RAFTDepthNormalDPT5(\n",
       "      (token2feature): EncoderFeature(\n",
       "        (read_3): Token2Feature(\n",
       "          (readoper): Readout(\n",
       "            (project_patch): LoRALinear(in_features=1024, out_features=1024, bias=True)\n",
       "            (project_learn): LoRALinear(in_features=5120, out_features=1024, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (sample): Identity()\n",
       "        )\n",
       "        (read_2): Token2Feature(\n",
       "          (readoper): Readout(\n",
       "            (project_patch): LoRALinear(in_features=1024, out_features=1024, bias=True)\n",
       "            (project_learn): LoRALinear(in_features=5120, out_features=1024, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (sample): Identity()\n",
       "        )\n",
       "        (read_1): Token2Feature(\n",
       "          (readoper): Readout(\n",
       "            (project_patch): LoRALinear(in_features=1024, out_features=1024, bias=True)\n",
       "            (project_learn): LoRALinear(in_features=5120, out_features=1024, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (sample): ConvTranspose2dLoRA(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (read_0): Token2Feature(\n",
       "          (readoper): Readout(\n",
       "            (project_patch): LoRALinear(in_features=1024, out_features=1024, bias=True)\n",
       "            (project_learn): LoRALinear(in_features=5120, out_features=1024, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (sample): Sequential(\n",
       "            (0): Conv2dLoRA(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder_mono): DecoderFeature(\n",
       "        (upconv_3): FuseBlock(\n",
       "          (way_trunk): ConvBlock(\n",
       "            (act): ReLU(inplace=True)\n",
       "            (conv1): Conv2dLoRA(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2dLoRA(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (out_conv): Conv2dLoRA(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (upconv_2): FuseBlock(\n",
       "          (way_trunk): ConvBlock(\n",
       "            (act): ReLU(inplace=True)\n",
       "            (conv1): Conv2dLoRA(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2dLoRA(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (way_branch): ConvBlock(\n",
       "            (act): ReLU(inplace=True)\n",
       "            (conv1): Conv2dLoRA(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2dLoRA(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (out_conv): Conv2dLoRA(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (upconv_1): FuseBlock(\n",
       "          (way_trunk): ConvBlock(\n",
       "            (act): ReLU(inplace=True)\n",
       "            (conv1): Conv2dLoRA(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2dLoRA(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (way_branch): ConvBlock(\n",
       "            (act): ReLU(inplace=True)\n",
       "            (conv1): Conv2dLoRA(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2dLoRA(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (out_conv): Conv2dLoRA(512, 258, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (depth_regressor): Sequential(\n",
       "        (0): Conv2dLoRA(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2dLoRA(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (normal_predictor): Sequential(\n",
       "        (0): Conv2dLoRA(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2dLoRA(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Conv2dLoRA(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2dLoRA(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (context_feature_encoder): ContextFeatureEncoder(\n",
       "        (outputs04): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): ResidualBlock(\n",
       "              (conv1): Conv2dLoRA(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (norm1): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2dLoRA(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (outputs08): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): ResidualBlock(\n",
       "              (conv1): Conv2dLoRA(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (norm1): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2dLoRA(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (outputs16): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): ResidualBlock(\n",
       "              (conv1): Conv2dLoRA(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (norm1): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2dLoRA(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): LayerNorm2d((128,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (context_zqr_convs): ModuleList(\n",
       "        (0-2): 3 x Conv2dLoRA(128, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (update_block): BasicMultiUpdateBlock(\n",
       "        (gru08): ConvGRU(\n",
       "          (convz): Conv2dLoRA(262, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (convr): Conv2dLoRA(262, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (convq): Conv2dLoRA(262, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (gru16): ConvGRU(\n",
       "          (convz): Conv2dLoRA(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (convr): Conv2dLoRA(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (convq): Conv2dLoRA(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (gru32): ConvGRU(\n",
       "          (convz): Conv2dLoRA(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (convr): Conv2dLoRA(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (convq): Conv2dLoRA(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (flow_head): FlowHead(\n",
       "          (conv1d): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2d): Conv2dLoRA(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv1n): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2n): Conv2dLoRA(128, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (mask): Sequential(\n",
       "          (0): Conv2dLoRA(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2dLoRA(128, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_large = Config.fromfile('./mono/configs/HourglassDecoder/vit.raft5.large.py')\n",
    "model_large = get_configured_monodepth_model(cfg_large, )\n",
    "model_large, _,  _, _ = load_ckpt('./weight/metric_depth_vit_large_800k.pth', model_large, strict_match=False)\n",
    "model_large.eval()\n",
    "device = \"cuda\"\n",
    "# device = \"cpu\"\n",
    "model_large.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_depth_normal(img, fx=1000.0, fy=1000.0, state_cache={},model = model_large,cfg = cfg_large):\n",
    "    \"\"\"\n",
    "    Predict depth map and normal map from input image, camera intrinsic\n",
    "\n",
    "    \"\"\"\n",
    "    # if model_selection == \"vit-small\":\n",
    "    #     model = model_small\n",
    "    #     cfg = cfg_small\n",
    "    # elif model_selection == \"vit-large\":\n",
    "    #     model = model_large\n",
    "    #     cfg = cfg_large\n",
    "\n",
    "    # else:\n",
    "    #     return None, None, None, None, state_cache, \"Not implemented model.\"\n",
    "    \n",
    "    if img is None:\n",
    "        return None, None, None, None, state_cache, \"Please upload an image and wait for the upload to complete.\"\n",
    "\n",
    "    model = model_large\n",
    "    cfg = cfg_large\n",
    "    cv_image = np.array(img) \n",
    "    img = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # intrinsic = [fx, fy, cx, cy]\n",
    "    intrinsic = [fx, fy, img.shape[1]/2, img.shape[0]/2]   #Assuming cx and cy are half of image width and height\n",
    "    \n",
    "    rgb_input, cam_models_stacks, pad, label_scale_factor = transform_test_data_scalecano(img, intrinsic, cfg.data_basic)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_depth, pred_depth_scale, scale, output, _ = get_prediction(\n",
    "                    model = model,\n",
    "                    input = rgb_input,\n",
    "                    cam_model = cam_models_stacks,\n",
    "                    pad_info = pad,\n",
    "                    scale_info = label_scale_factor,\n",
    "                    gt_depth = None,\n",
    "                    normalize_scale = cfg.data_basic.depth_range[1],\n",
    "                    ori_shape=[img.shape[0], img.shape[1]],\n",
    "                )\n",
    "    pred_depth = pred_depth.squeeze().cpu().numpy()\n",
    "    pred_depth[pred_depth<0] = 0\n",
    "    # pred_color = gray_to_colormap(pred_depth)\n",
    "\n",
    "    pred_normal = output['normal_out_list'][0][:, :3, :, :] \n",
    "    H, W = pred_normal.shape[2:]\n",
    "    pred_normal = pred_normal[:, :, pad[0]:H-pad[1], pad[2]:W-pad[3]]\n",
    "    pred_normal = torch.nn.functional.interpolate(pred_normal, [img.shape[0], img.shape[1]], mode='bilinear').squeeze()\n",
    "    pred_normal = pred_normal.permute(1,2,0)\n",
    "    pred_normal = pred_normal.cpu().numpy()\n",
    "\n",
    "    return  pred_depth, pred_depth_scale, scale, pred_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcd(H, W, depth_map, cx, cy, fx, fy):\n",
    "    \"\"\"\n",
    "    Calculate x,y,z for 3d point cloud using camera intrinsics and depth map\n",
    "    \n",
    "    \"\"\"\n",
    "    x_row = np.arange(0, W)\n",
    "    x = np.tile(x_row, (H, 1))\n",
    "    x = x.astype(np.float32)\n",
    "    u_m_cx = x - cx\n",
    "\n",
    "    y_col = np.arange(0, H)  \n",
    "    y = np.tile(y_col, (W, 1)).T\n",
    "    y = y.astype(np.float32)\n",
    "    v_m_cy = y - cy\n",
    "\n",
    "    x = u_m_cx / fx\n",
    "    y = v_m_cy / fy\n",
    "    z = np.ones_like(x)\n",
    "    pw = np.stack([x, y, z], axis=2)  \n",
    "    pcd = depth_map[:, :, None] * pw\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    img = Image.open(\"data\\sample_input\\IMG20240610231531.jpg\")\n",
    "    #Default values of fx and fy, actial values calculated from camera calibration.\n",
    "    fx=1000.0\n",
    "    fy=1000.0\n",
    "    voxel_down=0.02\n",
    "    #Depth Map\n",
    "    depth_map,pred_depth_scale,scale,pred_normal=predict_depth_normal(img, fx=fx, fy=fy,\n",
    "                                                                       state_cache={},model=model_large,cfg=cfg_large)\n",
    "    \n",
    "    # Scale Down the depth map and correspondingly other features to reduce the overall number of points in point cloud\n",
    "    if isinstance(depth_map, (np.ndarray) ):\n",
    "        img=np.array(img)\n",
    "        #intrinsic = [fx, fy, cx, cy] \n",
    "        intrinsic = [fx, fy, img.shape[1]/2, img.shape[0]/2]  #Assuming cx and cy are half of image width and height\n",
    "        if depth_map.shape[0] > 1080:\n",
    "                scale = 1080 / depth_map.shape[0]\n",
    "                depth_map = cv2.resize(depth_map, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "                img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "                pred_normal = cv2.resize(pred_normal, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "                intrinsic = [intrinsic[0]*scale, intrinsic[1]*scale, intrinsic[2]*scale, intrinsic[3]*scale]\n",
    "                \n",
    "    ##Point Cloud Creation\n",
    "    #Points Creation 2d array\n",
    "        if type(depth_map) == torch.__name__:\n",
    "            depth_map = depth_map.cpu().numpy().squeeze()\n",
    "        #Reduce noise in depth map\n",
    "        depth_map = cv2.medianBlur(depth_map, 5)\n",
    "        H, W = depth_map.shape\n",
    "        \n",
    "        pcd = get_pcd(H, W, depth_map, intrinsic[2], intrinsic[3], intrinsic[0], intrinsic[1])\n",
    "\n",
    "       \n",
    "    else:\n",
    "         print(\"Point Cloud Not Created\")\n",
    "        \n",
    "\n",
    "    #Points Color and Normal reshaping as 1d array\n",
    "    pcd_filtered = pcd.reshape(-1, 3)\n",
    "    img_filtered = img.reshape(-1, 3)\n",
    "    pred_normal=pred_normal.reshape(-1,3)\n",
    "    \n",
    "    #o3d point cloud creation \n",
    "    pcd2 = o3d.geometry.PointCloud()\n",
    "    pcd2.points = o3d.utility.Vector3dVector(pcd_filtered)\n",
    "    pcd2.colors = o3d.utility.Vector3dVector(np.array(img_filtered)/255.0)\n",
    "    pcd2.normals = o3d.utility.Vector3dVector(pred_normal)\n",
    "\n",
    "    # pcd2 = pcd2.voxel_down_sample(voxel_size=voxel_down)\n",
    "    o3d.visualization.draw_geometries([pcd2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "o3d.io.write_point_cloud(\"data\\sample_output\\IMG20240610231531.ply\", pcd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
